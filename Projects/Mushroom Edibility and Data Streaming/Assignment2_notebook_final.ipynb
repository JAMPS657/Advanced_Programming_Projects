{"cells":[{"cell_type":"code","source":["#author: Andrew J. Otis"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f79f8781-dc84-4df7-b954-9a4734eee61d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Recommended Modules\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\n# ------------------MACHINE LEARNING SECTION------------------\n# Load the dataset\nmushroom = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data')\n\n# Define column names mapping\ncol_map = {\"p\": \"mushroom_classification\", \"x\": \"cap-shape\", \"s\": \"cap-surface\"}\nmushroom = mushroom.rename(columns=col_map)\n\n# Select relevant columns\nmushroom = mushroom.iloc[:, :3]\n\n# Select features and output variable\nX = mushroom.drop('mushroom_classification', axis=1)\ny = mushroom['mushroom_classification']\n\n# Encode the target variable(i.e. \"mushroom_classification\")\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Split the data into training and test sets \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Pre-processing part of the pipeline\npreprocessing = ColumnTransformer([\n    ('onehot', OneHotEncoder(), ['cap-shape', 'cap-surface'])\n], remainder='passthrough')\n\n# Create a pipeline for a Random Forest Classifier\npipeline_rf = Pipeline([\n    ('preprocessing', preprocessing),\n    ('classification', RandomForestClassifier(max_depth=1, max_features=4))\n])\n\n# Fit the pipeline on the training data\npipeline_rf.fit(X_train, y_train)\n\n# Evaluate the pipeline on the training set\nprint(\"----Random Forest Accuracy Results----\")\ntrain_accuracy = pipeline_rf.score(X_train, y_train)\nprint(\"Accuracy on training set:\", train_accuracy)\n\n# Evaluate the pipeline on the test set\ntest_accuracy = pipeline_rf.score(X_test, y_test)\nprint(\"Accuracy on test set:\", test_accuracy)\n\n# Create another pipeline for Decision Tree Classifier\npipeline_dt = Pipeline([\n    ('preprocessing', preprocessing),\n    ('classification', DecisionTreeClassifier(max_depth=1, max_features=4))\n])\n\n# Fit the pipeline on the training data\npipeline_dt.fit(X_train, y_train)\n\n# Evaluate the pipeline on the training set\nprint()\nprint(\"----Decision Tree Accuracy Results----\")\ntrain_accuracy2 = pipeline_dt.score(X_train, y_train)\nprint(\"Accuracy on training set:\", train_accuracy2)\n\n# Evaluate the pipeline on the test set\ntest_accuracy2 = pipeline_dt.score(X_test, y_test)\nprint(\"Accuracy on test set:\", test_accuracy2)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7f889219-7878-48ca-b06e-660c8e00f62e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["----Random Forest Accuracy Results----\nAccuracy on training set: 0.619416109743229\nAccuracy on test set: 0.6175625769388593\n\n----Decision Tree Accuracy Results----\nAccuracy on training set: 0.5160042208934225\nAccuracy on test set: 0.5227739023389413\n"]}],"execution_count":0},{"cell_type":"code","source":["# Recommended Modules\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark.streaming import StreamingContext\n\n# Important note: \n#    All the code assosciated with DB directory manipulation is commented out since I do not need to re-create the directories or re-copy them over. However, please reference this code if you wanted to create your own directory for the streaming dataset and another DB directory for the training and test sets.\n\n# DB Directory Manipulation for streaming data\n# Create a DB directory for the train and test sets\n#dbutils.fs.mkdirs(\"FileStore/tables/assignment2\")\n\n# Create another DB directory for the data to be streamed\n# Make a directory for relevant lab6 files\n#dbutils.fs.mkdirs(\"FileStore/tables/assignment2/streaming\")\n\n# Refer to the file \"HW2.py\" for the code that saves the train and test sets whole\n\n# Refer to the file \"HW2.py\" for the code that creates the batches of test data. These batch files will be in their own directory for the streaming portion of the report.\n\n# After saving CSVs of the batch data locally with the \"HW2.py\" and manually move them to the main DB directory  the following code is ran to copy multiple files from one directory to another(i.e. I'm moving the relevant files for streaming into their own directory)\n\nimport os\n#source_directory = \"dbfs:/FileStore/tables\"\n#destination_directory = \"dbfs:/FileStore/tables/assignment2/streaming\"\n\n# Create the destination directory if it doesn't exist\n#dbutils.fs.mkdirs(destination_directory)\n\n# List files in the source directory\n#files = dbutils.fs.ls(source_directory)\n\n# Copy files with names containing \"test_part..\" to the destination directory\n#for file in files:\n    #if \"test_part\" in file.name:\n        #source_path = os.path.join(source_directory, file.name)\n        #destination_path = os.path.join(destination_directory, file.name)\n        #dbutils.fs.cp(source_path, destination_path)\n        \n\n#----------------------------STREAMING SECTION-------------------------\n# Checking current files in new DataBricks directory\n#dbutils.fs.ls (\"dbfs:/FileStore/tables/assignment2/streaming\")\n\n# Create SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Define the schema for training data in order to train the ML model\ntrain_schema = StructType([\n    StructField('cap-shape', StringType(), True),\n    StructField('cap-surface', StringType(), True),\n    StructField('mushroom_classification', DoubleType(), True)\n])\n\n# Load the training data with the specified schema\ntrain_data = spark.read.format(\"csv\").schema(train_schema).option(\"header\", \"true\").load(\"dbfs:/FileStore/tables/assignment2/mushroom_training.csv\")\n\n# Define the features column\nfeatures_col = ['cap-shape', 'cap-surface']\n\n# Remove null values from the dataset\ntrain_data = train_data.na.drop()\n\n# Preprocessing transformer\nindexer = StringIndexer(inputCols=features_col, outputCols=['{}_indexed'.format(col) for col in features_col]).setHandleInvalid(\"keep\")\nencoder = OneHotEncoder(inputCols=['{}_indexed'.format(col) for col in features_col], outputCols=['{}_encoded'.format(col) for col in features_col])\nassembler = VectorAssembler(inputCols=['{}_encoded'.format(col) for col in features_col], outputCol='features')\n\ntrain_data.head(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0c8f0ce3-dd0f-484d-a375-0c1c154d7af2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[3]: [Row(cap-shape='f', cap-surface='y', mushroom_classification=1.0),\n Row(cap-shape='f', cap-surface='y', mushroom_classification=0.0),\n Row(cap-shape='f', cap-surface='f', mushroom_classification=0.0),\n Row(cap-shape='x', cap-surface='f', mushroom_classification=1.0),\n Row(cap-shape='f', cap-surface='f', mushroom_classification=0.0)]"]}],"execution_count":0},{"cell_type":"code","source":["# Define schema for incoming streaming data\nstreaming_schema = StructType([\n    StructField('cap-shape', StringType(), True),\n    StructField('cap-surface', StringType(), True),\n    StructField('mushroom_classification', DoubleType(), True)\n])\n\n# Random Forest Classifier Model\nrf_classifier = RandomForestClassifier(labelCol='mushroom_classification', featuresCol='features', maxDepth=1, maxBins=4)\n\n# Create the pipeline for Random Forest Classifier\nrf_pipeline = Pipeline(stages=[indexer, encoder, assembler, rf_classifier])\n\n# Fit the pipeline on the training data\nrf_pipeline_model = rf_pipeline.fit(train_data)\n\n# Define the testing directory\ntesting_directory = \"dbfs:/FileStore/tables/assignment2/streaming\"\n\n# Create a streaming DataFrame to read one file per trigger from the testing directory\nstreaming_df = (spark.readStream.format(\"csv\")\n                .option(\"header\", \"true\")\n                .schema(streaming_schema)\n                .option(\"maxFilesPerTrigger\", 1)\n                .load(testing_directory))\n\n# Remove null values from the streaming data\nstreaming_df = streaming_df.na.drop()\n\n# Apply the Random Forest model to the streaming data\nrf_predictions = rf_pipeline_model.transform(streaming_df).select(\"cap-shape\", \"cap-surface\", \"mushroom_classification\", \"prediction\")\n\n# Start the streaming query for Random Forest\nrf_query = rf_predictions.writeStream.format(\"memory\").outputMode(\"append\").queryName(\"rf_stream\").start()\n\n# Display the streaming results\ndisplay(spark.table(\"rf_stream\"))\n\n# Wait for the streaming queries to finish\n#rf_query.awaitTermination()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e7e18ba1-a422-4fa7-a451-8735ec88593e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"cap-shape","type":"\"string\"","metadata":"{}"},{"name":"cap-surface","type":"\"string\"","metadata":"{}"},{"name":"mushroom_classification","type":"\"double\"","metadata":"{}"},{"name":"prediction","type":"\"double\"","metadata":"{\"ml_attr\":{\"type\":\"nominal\",\"num_vals\":2}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cap-shape</th><th>cap-surface</th><th>mushroom_classification</th><th>prediction</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(labelCol='mushroom_classification', featuresCol='features', maxDepth=1, maxBins=4)\n\n# Create the pipeline for Decision Tree Classifier\ndt_pipeline = Pipeline(stages=[indexer, encoder, assembler, dt_classifier])\n\ndt_pipeline_model = dt_pipeline.fit(train_data)\n\n# Apply the Decision Tree model to the streaming data\ndt_predictions = dt_pipeline_model.transform(streaming_df).select(\"cap-shape\", \"cap-surface\", \"mushroom_classification\", \"prediction\")\n\n# Start the streaming query for Decision Tree\ndt_query = dt_predictions.writeStream.format(\"console\").outputMode(\"append\").queryName(\"dt_stream\").start()\n\n# Display the streaming predictions\ndisplay(\"dt_stream\")\n\n# Wait for the streaming queries to finish\n#dt_query.awaitTermination()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"32f2c9bd-96cc-4ea5-9cbe-0682341a740c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["'dt_stream'"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Assignment2_notebook_final","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
